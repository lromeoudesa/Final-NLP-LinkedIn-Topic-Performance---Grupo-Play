{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de tópicos\n",
    "\n",
    "Grupo Play: Manuel Brito, Ezequiel Ortiz Recalde, Lucas Romeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaciones necesarias para poder seguir el proceso\n",
    "\n",
    "#!pip install langdetect\n",
    "# correr en la terminal:\n",
    "# python -m nltk.downloader averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los paquetes necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10056, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('influencer_1_120.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeamos observaciones con reacciones deshabilitadas\n",
    "a = df[df['Reaction']=='Disable'].index\n",
    "df.drop(a,inplace=True)\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embedding'] = df['Embedding'].str.strip('[]')\n",
    "df['hashtag_q'] = df['Embedding'].str.findall(r'(hashtag)').map(len)\n",
    "df['company_q'] = df['Embedding'].str.findall(r'(/company/)').map(len)\n",
    "df['person_q'] = df['Embedding'].str.findall(r'(/in/|lnkd)').map(len)\n",
    "df.loc[df['Image'] != '0','Image'] = 1\n",
    "df.loc[df['LinkedIn_Video'] != '0','LinkedIn_Video'] = 1\n",
    "df.loc[df['External_Video'] != '0','External_Video'] = 1\n",
    "df.loc[df['Podcast'] != '0','Podcast'] = 1\n",
    "df.loc[df['Article'] != '0','Article'] = 1\n",
    "df.loc[df['Comment'] == 'Disable','Comment'] = '0'\n",
    "df['Comment'] = df['Comment'].str.extract(r'(\\d+)')\n",
    "df['Comment'] = df['Comment'].str.replace(r'.','')\n",
    "df['Comment'] = df['Comment'].astype(int)\n",
    "df['Followers'] = df['Followers'].str.replace(r',','')\n",
    "df['Followers'] = df['Followers'].str.replace(r'.','')\n",
    "df['Followers'] = df['Followers'].astype(int)\n",
    "df.loc[df['Reaction'] == 'Disable','Reaction'] = '0'\n",
    "df['Reaction'] = df['Reaction'].str.replace(r'.','')\n",
    "df['Reaction'] = df['Reaction'].str.replace(r',','')\n",
    "df['Reaction'] = df['Reaction'].astype(int)\n",
    "\n",
    "modelo = ['Name', 'Followers', 'Header', 'Comment',\n",
    "              'Reaction', 'Image', 'LinkedIn_Video', 'External_Video', 'Podcast',\n",
    "              'Article', 'hashtag_q', 'company_q', 'person_q']\n",
    "df = df[modelo]\n",
    "\n",
    "df.columns = ['Name', 'Followers', 'Text', 'Comment',\n",
    "              'Reaction', 'Image', 'LinkedIn_Video', 'External_Video', 'Podcast',\n",
    "              'Article', 'hashtag_q', 'company_q', 'person_q']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Limpieza del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de idiomas que no sean inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = []\n",
    "indices = []\n",
    "for i in df['Text']:\n",
    "    try:\n",
    "        lang.append(detect(i))\n",
    "    except:\n",
    "        lang.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro de posts que no están en ingles\n",
    "df['lang'] = lang\n",
    "other_lang = df[(df['lang']!=0) & (df['lang']!='en') & (df['lang']!='da') & (df['lang']!='ro') & (df['lang']!='ca')].index\n",
    "\n",
    "df.drop(other_lang,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Filtro adicional para palabras en francés que el detector no identificó\n",
    "df['validation']=df['Text'].str.extract(r'(joly|très|plaisir|dernier|Politique|pandémie|Mondiale|Economique|Tout|Quelle|depuis|Pour|fais|stratégie|suivez|Fais|cette|assez|vois|avec|Avec|Propriétaires|Vous| il y a|Pendant)')\n",
    "french = df.loc[df['validation'].notnull()].index\n",
    "df.drop(french,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9587, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidación de expresiones\n",
    "df['Text'] = df['Text'].str.replace(r'(COVID19|Covid-19|COVID-19|covid19|covid-19|COVID-19)','COVID')\n",
    "df['Text'] = df['Text'].str.replace(r'(working)','work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscamos los sustantivos y adjetivos, sacamos stopwords y lematizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation + '—’'\n",
    "def nouns_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word.lower() for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "def text_process(text):\n",
    "    stop_words = stopwords.words('english') + ['us','new','get','one','many','want','de','en','c','please',\\\n",
    "                                               'thank','thank','thanks','think','news',\\\n",
    "                                               'great','powerful','good',\\\n",
    "                                               'amazing','youre','ive','im','la','joly','de','en',\\\n",
    "                                               'year','today','day','also',\\\n",
    "                                               'month','would','will','paypal','could','wont'\\\n",
    "                                               'excited','dont',\\\n",
    "                                               \"we're\",'it’s','i’m','—','-','1',\\\n",
    "                                               '2','3','4','5','6','7','8','9','10',\\\n",
    "                                               'tomorrow','monday','tuesday','wednesay','thursday',\\\n",
    "                                               'friday','saturday','sunday','going','everyone','join',\\\n",
    "                                               'others','know','made','make','like','see','take','thats',\\\n",
    "                                               'well','way','week','look','looking','come','linkedin']\n",
    "    \n",
    "    nopunc = [char for char in text if char not in string.punctuation] \n",
    "    nopunc = ''.join(nopunc)\n",
    "    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in set(stop_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_Text'] = df['Text'].apply(nouns_adj)\n",
    "df['clean_Text'] = df['Text'].apply(text_process)\n",
    "df['clean_Text'] = df['clean_Text'].apply(lambda x: ' '.join(WordNetLemmatizer().lemmatize(term) for term in x.split()))\n",
    "df['clean_Text'] = df['clean_Text'].str.replace(r'(leadership)','leader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matriz de tokens y LDA\n",
    "Reducimos la dimensionalidad requiriendo que los tokens aparezcan en al menos el 3% del total de todos los posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',min_df=0.03,token_pattern='[a-zA-Z0-9]{3,}')\n",
    "data_vectorized = vectorizer.fit_transform(df['clean_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(learning_method='online',\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 max_iter=5, random_state=0),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [3, 4, 5]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos distintos learning_decay y numeros de componentes\n",
    "params = {'learning_decay': [.5, .7, .9],'n_components': [3,4,5]}\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "model = GridSearchCV(lda, param_grid=params)\n",
    "model.fit(data_vectorized)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 3}\n",
      "Model Perplexity:  66.2533271514195\n"
     ]
    }
   ],
   "source": [
    "# Mejor modelo\n",
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcanzamos el mejor resultado con 3 componentes. Procedemos a ver su composición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Matrix a dataframe\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "topicnames = ['Topic' + str(i) for i in range(best_lda_model.n_components)]\n",
    "docnames = [str(i) for i in range(len(df['Text']))]\n",
    "adf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "dominant_topic = np.argmax(adf_document_topic.values, axis=1)\n",
    "adf_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N Words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>leader</td>\n",
       "      <td>woman</td>\n",
       "      <td>team</td>\n",
       "      <td>work</td>\n",
       "      <td>ceo</td>\n",
       "      <td>community</td>\n",
       "      <td>forward</td>\n",
       "      <td>support</td>\n",
       "      <td>together</td>\n",
       "      <td>live</td>\n",
       "      <td>opportunity</td>\n",
       "      <td>excited</td>\n",
       "      <td>read</td>\n",
       "      <td>industry</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>time</td>\n",
       "      <td>business</td>\n",
       "      <td>company</td>\n",
       "      <td>world</td>\n",
       "      <td>year</td>\n",
       "      <td>first</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>global</td>\n",
       "      <td>technology</td>\n",
       "      <td>important</td>\n",
       "      <td>love</td>\n",
       "      <td>thing</td>\n",
       "      <td>right</td>\n",
       "      <td>every</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>people</td>\n",
       "      <td>help</td>\n",
       "      <td>work</td>\n",
       "      <td>share</td>\n",
       "      <td>book</td>\n",
       "      <td>need</td>\n",
       "      <td>change</td>\n",
       "      <td>future</td>\n",
       "      <td>learn</td>\n",
       "      <td>next</td>\n",
       "      <td>employee</td>\n",
       "      <td>health</td>\n",
       "      <td>better</td>\n",
       "      <td>best</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 0    Word 1   Word 2 Word 3 Word 4     Word 5    Word 6  \\\n",
       "Topic 0  leader     woman     team   work    ceo  community   forward   \n",
       "Topic 1    time  business  company  world   year      first  pandemic   \n",
       "Topic 2  people      help     work  share   book       need    change   \n",
       "\n",
       "          Word 7      Word 8     Word 9      Word 10  Word 11 Word 12  \\\n",
       "Topic 0  support    together       live  opportunity  excited    read   \n",
       "Topic 1   global  technology  important         love    thing   right   \n",
       "Topic 2   future       learn       next     employee   health  better   \n",
       "\n",
       "          Word 13 Word 14  \n",
       "Topic 0  industry  career  \n",
       "Topic 1     every    work  \n",
       "Topic 2      best   covid  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top N Palabras por topico\n",
    "def show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)\n",
    "\n",
    "# Topic words df\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple vista, consideramos que se podrían etiquetar los tópicos de la siguiente forma:\n",
    "- Topic 0: Tendencias\n",
    "- Topic 1: Negocios\n",
    "- Topic 2: Responsabilidad social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020', 'around', 'back', 'best', 'better', 'book', 'business',\n",
       "       'career', 'ceo', 'challenge', 'change', 'community', 'company',\n",
       "       'conversation', 'country', 'covid', 'customer', 'economy', 'employee',\n",
       "       'even', 'every', 'excited', 'experience', 'feel', 'find', 'first',\n",
       "       'forward', 'friend', 'future', 'global', 'group', 'health', 'help',\n",
       "       'important', 'industry', 'job', 'last', 'leader', 'learn', 'life',\n",
       "       'live', 'love', 'much', 'need', 'next', 'opportunity', 'organization',\n",
       "       'pandemic', 'part', 'partner', 'people', 'proud', 'question', 'read',\n",
       "       'right', 'role', 'share', 'support', 'team', 'technology', 'thing',\n",
       "       'thought', 'time', 'together', 'virtual', 'woman', 'work', 'world',\n",
       "       'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic words matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "# Columna e índice\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "df_topic_keywords.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consolidación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la configuración de la matriz de tokens, era de esperar que algunas observaciones con palabras poco frecuentes no posean un tópico definido. A continuación las descartamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservamos los índices de las observaciones con tópico indefinido\n",
    "x = adf_document_topic[(adf_document_topic['Topic0']==0.33)&(adf_document_topic['Topic1']==0.33)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame(adf_document_topic)\n",
    "df_topics.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics['index']=df_topics['index'].astype(int)\n",
    "df_final = pd.merge(df1,df_topics,on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Name', 'Followers', 'Text', 'Comment', 'Reaction', 'Image','LinkedIn_Video', 'External_Video', 'Podcast', 'Article',\n",
    "            'hashtag_q','company_q', 'person_q','Topic0','Topic1', 'Topic2', 'dominant_topic']\n",
    "df_final=df_final[columnas]\n",
    "\n",
    "x = x.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(x,inplace=True)\n",
    "df_final.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature que indica la cantidad de caracteres\n",
    "df_final['text_len']=df_final['Text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['topic_proba']=df_final[['Topic0','Topic1','Topic2']].apply(max,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns\n",
    "columnas = ['Name', 'Followers', 'Text', 'Comment', 'Reaction', 'Image',\n",
    "            'LinkedIn_Video', 'External_Video', 'Podcast', 'Article', 'hashtag_q',\n",
    "            'company_q', 'person_q', 'dominant_topic', 'topic_proba','text_len']\n",
    "\n",
    "df_final=df_final[columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asignación de etiquetas a tópicos\n",
    "\n",
    "Asignamos las etiquetas a los tópicos en función de sus palabras más frecuentes, resaltando que esta no es la única forma de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[df_final['dominant_topic'] == 0,'dominant_topic']= 'tendencias'\n",
    "df_final.loc[df_final['dominant_topic'] == 1,'dominant_topic']= 'negocios'\n",
    "df_final.loc[df_final['dominant_topic'] == 2,'dominant_topic']= 'responsabilidad social'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset final\n",
    "\n",
    "Guardamos el resultado en un dataset para poder utilizarlo en la siguiente parte del proceso: el armado de clusters y su caracterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final = pd.read_csv('dataset3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
