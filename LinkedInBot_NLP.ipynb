{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, random, sys, time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abrimos una pestaña en el browser que será comandada de manera automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('driver/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Abrimos la página de LinkedIn y nos logeamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://www.linkedin.com/uas/login') #carga de página\n",
    "file = open('config.txt') #se debe configurar un path a un txt con el usuario y la contraseña\n",
    "lines = file.readlines() #leemos el archivo\n",
    "username = lines[0] #Obtenemos el usuario\n",
    "password = lines[1] #Obtenemos la contraseña\n",
    "elementID = browser.find_element_by_id('username') #Buscamos el campo donde introducir el username\n",
    "elementID.send_keys(username) #Pasamos el usuario\n",
    "elementID = browser.find_element_by_id('password') #Buscamos el campo donde introducir el username\n",
    "elementID.send_keys(password) #Pasamos la contraseña\n",
    "elementID.submit() #Apretamos el boton de login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creamos el scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(full_link,n_post): #los argumentos de la función son los perfiles a explorar y el número de post\n",
    "    n_scroll = max(int(n_post/5 - 1),1) #En base al número de post calculamos cuantos scrolls debemos realizar en la página\n",
    "    fullLink = full_link + 'detail/recent-activity/' #Añadimos un sufijo al link para que nos lleve directamente a la actividad del perfil\n",
    "    browser.get(fullLink) #Obtenemos la información de la página\n",
    "\n",
    "    # Scroll down en la página web\n",
    "    last_height = browser.execute_script('return document.body.scrollHeight') #Le pedimos obtener la información de la página\n",
    "    for i in range(n_scroll): #Recorremos el número de scrolls \n",
    "        browser.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "        time.sleep(random.uniform(5,7)) #Durante el scroll dormimos por unos segundos al bot\n",
    "        new_height = browser.execute_script('return document.body.scrollHeight') #Guardamos la información extraída\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "\n",
    "    # Busco la sección del header\n",
    "    followers_div = soup.find('div',{'class':'flex-1 text-align-right t-14 t-black t-light'})\n",
    "    followers = followers_div.get_text().strip()\n",
    "    name_div = soup.find('h3',{'class':'single-line-truncate t-16 t-black t-bold mt2'})\n",
    "    name = name_div.get_text().strip()\n",
    "    text_div = soup.find_all('div',{'class':'feed-shared-update-v2__description-wrapper ember-view'})\n",
    "    interaction_div = soup.find_all('ul',{'class':'social-details-social-counts ember-view'})\n",
    "    post_div = soup.find_all('div',{'class':'occludable-update ember-view'})\n",
    "    \n",
    "    # Name and followers\n",
    "    name_list = [name]*len(text_div)\n",
    "    follower_list = [followers]*len(text_div)\n",
    "\n",
    "    # Otras variables\n",
    "    embedding = []\n",
    "    header = []\n",
    "    reaction = []\n",
    "    comment = []\n",
    "    image_cont = []\n",
    "    linkedin_video_cont = []\n",
    "    external_video_cont = []\n",
    "    podcast_cont = []\n",
    "    article_cont = []\n",
    "\n",
    "    for i in range(0,len(text_div)):\n",
    "        \n",
    "        # text\n",
    "        try:\n",
    "            text = text_div[i].find('span',{'dir':'ltr'})\n",
    "        except:\n",
    "            text = 0\n",
    "            \n",
    "        # embeddings\n",
    "        try:\n",
    "            embed = text.find_all('a', href=True)\n",
    "            embedding.append(''.join(str(embed)))\n",
    "        except:\n",
    "            embedding.append(0)\n",
    "        \n",
    "        # header\n",
    "        try:\n",
    "            head = text.get_text().strip()\n",
    "            header.append(head)\n",
    "        except:\n",
    "            header.append(0)\n",
    "            \n",
    "        # reactions\n",
    "        try:\n",
    "            reaction_div = interaction_div[i].find('span',{'class':'v-align-middle social-details-social-counts__reactions-count'})\n",
    "            reaction.append(reaction_div.get_text().strip())\n",
    "        except:\n",
    "            reaction.append('Disable')\n",
    "        \n",
    "        # comments\n",
    "        try:\n",
    "            comment_div = interaction_div[i].find_all('span',{'class':'v-align-middle'})[1]\n",
    "            comment.append(comment_div.get_text().strip(' Comments'))\n",
    "        except:\n",
    "            comment.append('Disable')\n",
    "            \n",
    "        # image\n",
    "        try:\n",
    "            image = re.search(r'(feed-shared-image)',str(post_div[i]))[0]\n",
    "            image_cont.append(image)\n",
    "        except:\n",
    "            image_cont.append(0)\n",
    "        # linkedin_video\n",
    "        try:\n",
    "            linkedin_video = re.search(r'(feed-shared-linkedin-video)',str(post_div[i]))[0]\n",
    "            linkedin_video_cont.append(linkedin_video)\n",
    "        except:\n",
    "            linkedin_video_cont.append(0)\n",
    "        # external_video\n",
    "        try:\n",
    "            external_video = re.search(r'(feed-shared-external-video)',str(post_div[i]))[0]\n",
    "            external_video_cont.append(external_video)\n",
    "        except:\n",
    "            external_video_cont.append(0)\n",
    "        # podcast\n",
    "        try:\n",
    "            podcast = re.search(r'(podcast)',str(post_div[i]))[0]\n",
    "            podcast_cont.append(podcast)\n",
    "        except:\n",
    "            podcast_cont.append(0)\n",
    "        # article\n",
    "        try:\n",
    "            article = re.search(r'(article)',str(post_div[i]))[0]\n",
    "            article_cont.append(article)\n",
    "        except:\n",
    "            article_cont.append(0)\n",
    "\n",
    "    \n",
    "    # Creamos un data frame con los datos\n",
    "    df = pd.DataFrame({'Name':name_list,\n",
    "                       'Followers':follower_list,\n",
    "                       'Embedding': embedding,\n",
    "                       'Header': header,\n",
    "                       'Comment': comment,\n",
    "                       'Reaction':reaction,\n",
    "                       'Image':image_cont,\n",
    "                       'LinkedIn_Video':linkedin_video_cont,\n",
    "                       'External_Video':external_video_cont,\n",
    "                       'Podcast':podcast_cont,\n",
    "                       'Article':article_cont})\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Leemos el archivo que tiene el listado de links con los perfiles de los influencers de LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv('url.csv')\n",
    "list_links = df_links.url.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Ejecutamos el scrapper y recorremos la lista de influencers extrayendo sus posteos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_final = pd.DataFrame({'Name':[],'Followers':[],'Embedding': [],'Header': [],'Comment': [],\n",
    "                         'Reaction':[],'Image':[],'LinkedIn_Video':[],'External_Video':[],'Podcast':[],'Article':[]})\n",
    "for i in list_links:\n",
    "    df = feature_extraction(i,100)\n",
    "    df_final = pd.concat([df_final,df])\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Guardamos el dataframe obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('influencer_1_100.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
